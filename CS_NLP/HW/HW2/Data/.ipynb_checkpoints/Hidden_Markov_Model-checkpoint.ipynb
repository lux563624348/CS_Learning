{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_file(file_path, num_rows):\n",
    "    ## num_rows has to be a 2 elements list\n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        row_range=range(num_rows[0],num_rows[1]+1)\n",
    "        i=0\n",
    "        for line in file:\n",
    "            if (i in row_range):               \n",
    "                   print (line)\n",
    "            i+=1\n",
    "    return None\n",
    "\n",
    "def filter_alphanumeric(word):\n",
    "#\\w matches any alphanumeric character\n",
    "    merge_words_no_digit=''\n",
    "    if (word!=''):\n",
    "        all_match = re.findall('\\w+', word)\n",
    "        all_match = list(filter(None, all_match))\n",
    "        merge_words=''\n",
    "        for item in all_match:\n",
    "            merge_words+=item\n",
    "        ## Find All unicode, then all non digits\n",
    "        merge_words=re.findall('[^(\\_|\\d)]', merge_words)\n",
    "        for item in merge_words:\n",
    "            merge_words_no_digit+=item        \n",
    "    else:\n",
    "        #print (\"Warning, One word is empty.\")\n",
    "        return '' \n",
    "    return merge_words_no_digit.lower()\n",
    "\n",
    "def return_word_list_from_file(Path_File):\n",
    "    list_words=list()\n",
    "    with open(Path_File,  mode='r', newline='') as file:\n",
    "        for line in file:\n",
    "            for word in (re.split(\"\\s+\", line.rstrip('\\n'))):\n",
    "                if (word !=''):\n",
    "                    list_words.append(filter_alphanumeric(word))\n",
    "## if different languague, above line has to be changed\n",
    "    list_words = list(filter(None, list_words))\n",
    "    return list_words\n",
    "\n",
    "\n",
    "def return_sentence_list_from_file(Path_File):\n",
    "    list_sentences=list()\n",
    "    with open(Path_File,  mode='r', newline='') as file:\n",
    "        for line in file:\n",
    "            if (len(line)>1):\n",
    "                sentence = line.rstrip('\\n').rstrip('\\r')\n",
    "                #marked_sentence = #'<^> '+line.rstrip('\\n').rstrip('\\r')+' </s>'\n",
    "                list_sentences.append(sentence)\n",
    "    list_sentences = list(filter(None, list_sentences))\n",
    "    return list_sentences\n",
    "\n",
    "\n",
    "## not needed in HMM\n",
    "def return_vocabulary_from_sentence_list(sentence_list):\n",
    "    total_vocabulary=set({'^','$'})\n",
    "    for sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "        for word in word_list:\n",
    "            filtered_word = filter_alphanumeric(word)\n",
    "            if(filtered_word!=''):\n",
    "                total_vocabulary.add(filtered_word)\n",
    "    return sorted(list(total_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unigram_counts(sentence_list, vocabulary):\n",
    "    count_matrix =np.zeros((len(vocabulary)))\n",
    "    count_matrix += len(vocabulary) ######## Add-one smoothing\n",
    "    ## Set value for sentence start <s>\n",
    "    count_matrix[vocabulary.index('^')] += len(sentence_list)\n",
    "    count_matrix[vocabulary.index('$')] += len(sentence_list)\n",
    "    for tem_sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", tem_sentence.rstrip('\\n'))\n",
    "        for word in word_list:\n",
    "            filtered_word = filter_alphanumeric(word)\n",
    "            if(filtered_word in vocabulary):\n",
    "                word_index = vocabulary.index(filtered_word)\n",
    "                count_matrix[word_index]+=1\n",
    "    return count_matrix\n",
    "\n",
    "def Return_c_from_Good_Turing_Smoothing(unigram_count_matrix):\n",
    "    N_1=0\n",
    "    N_2=0\n",
    "    N_words = unigram_count_matrix.sum()\n",
    "    for x in unigram_count_matrix:\n",
    "        if(x==1):\n",
    "            N_1+=1\n",
    "        elif(x==2):\n",
    "            N_2+=1\n",
    "        else:\n",
    "            continue\n",
    "    c_for_zero = 1.0*N_1/N_words\n",
    "    c_for_once  = 1.0*(1+1)*(N_2/N_1)\n",
    "    return c_for_zero\n",
    "\n",
    "\n",
    "def return_bigram_word_counts(sentence_list, vocabulary):\n",
    "    count_matrix=np.zeros((len(vocabulary),len(vocabulary)))\n",
    "    #count_matrix+=1  ######## Add-one smoothing\n",
    "    num_word=0\n",
    "    for tem_sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", tem_sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))\n",
    "        \n",
    "        for i in range(0,len(word_list)-1):\n",
    "            num_word+=1\n",
    "            if (i==0):\n",
    "                first_word = filter_alphanumeric(word_list[0])\n",
    "                if(first_word in vocabulary):\n",
    "                    count_matrix[vocabulary.index('^'), vocabulary.index(first_word)] += 1\n",
    "            tem_bigram_count_pairs = word_list[i:i+2]\n",
    "            first_word=filter_alphanumeric(tem_bigram_count_pairs[0])\n",
    "            second_word=filter_alphanumeric(tem_bigram_count_pairs[1])\n",
    "            \n",
    "            if((first_word in vocabulary) & (second_word in vocabulary)):\n",
    "                first_digit=vocabulary.index(first_word)\n",
    "                second_digit=vocabulary.index(second_word)\n",
    "                count_matrix[first_digit,second_digit]+=1\n",
    "    print (\"Number of Words: \"+ str(num_word))\n",
    "    return count_matrix\n",
    "\n",
    "def return_Trigram_word_counts(sentence_list, vocabulary):\n",
    "    count_matrix=np.zeros((len(vocabulary),len(vocabulary)))\n",
    "    #count_matrix+=1  ######## Add-one smoothing\n",
    "    num_word=0\n",
    "    for tem_sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", tem_sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))\n",
    "        \n",
    "        for i in range(0,len(word_list)-2):\n",
    "            num_word+=1\n",
    "            if (i==0):\n",
    "                first_word = filter_alphanumeric(word_list[0])\n",
    "                second_word = filter_alphanumeric(word_list[1])\n",
    "                if((first_word in vocabulary) & ((second_word in vocabulary))):\n",
    "                    count_matrix[vocabulary.index(first_word), vocabulary.index(second_word)] += 1\n",
    "            tem_trigram_count_chain = word_list[i:i+3]\n",
    "            pre_two_words=tem_trigram_count_chain[0:2]\n",
    "            \n",
    "            third_word=filter_alphanumeric(tem_trigram_count_chain[2])\n",
    "            if((third_word in vocabulary)):\n",
    "                first_digit=vocabulary.index(pre_two_words)\n",
    "                second_digit=vocabulary.index(third_word)\n",
    "                count_matrix[first_digit,second_digit]+=1\n",
    "    print (\"Number of Words: \"+ str(num_word))\n",
    "    return count_matrix\n",
    "\n",
    "\n",
    "def Norm_Bigram(unigram_counts, bigram_counts, vocabulary):\n",
    "    #c_0 = Return_c_from_Good_Turing_Smoothing(unigram_counts)\n",
    "    c_0=1.0\n",
    "    norm_bigram = np.zeros((len(vocabulary),len(vocabulary))) \n",
    "    for i in range(len(vocabulary)):\n",
    "        norm_bigram[i,:]=(bigram_counts[i,:]+c_0)/(unigram_counts[i])\n",
    "    return norm_bigram\n",
    "\n",
    "def Norm_Trigram(unigram_counts, bigram_counts, trigram_counts, vocabulary):\n",
    "    #c_0 = Return_c_from_Good_Turing_Smoothing(unigram_counts)\n",
    "    norm_trigram = np.zeros((len(vocabulary),len(vocabulary)))\n",
    "    for i in range(len(vocabulary)):\n",
    "            norm_trigram[i,:]=(norm_trigram[i,:]+c_0)/(bigram_counts[i])\n",
    "    return norm_trim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'brown.test.tagged.txt',\n",
       " 'brown.test.txt',\n",
       " 'brown.train.tagged.raw',\n",
       " 'brown.train.tagged.txt',\n",
       " 'Hidden_Markov_Model.ipynb']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``/`` He/pps almost/rb brought/vbd it/ppo back/rb all/abn the/at way/nn ''/'' ./. \n",
      "\n",
      "There/ex is/bez a/at vast/jj difference/nn between/in the/at community/nn of/in reconciliation/nn which/wdt the/at New/jj Testament/nn describes/vbz and/cc the/at community/nn of/in congeniality/nn found/vbn in/in the/at average/jj church/nn building/nn ./. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#head_file('brown.train.tagged.raw',[0,3])\n",
    "head_file('brown.train.tagged.txt',[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-360-f3f2be390cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0msentence_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;31m#[0:10000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mtag_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_Tagset_from_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-360-f3f2be390cb5>\u001b[0m in \u001b[0;36mreturn_Tagset_from_train_data\u001b[0;34m(sentence_list)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_tag_from_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mtotal_Tagset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mtotal_Tagset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_Tagset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_Tagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_Tagset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "def return_tag_from_words(word):\n",
    "    # tagging_mark = \"/\"\n",
    "    split_word_set = word.split(\"/\")\n",
    "    return split_word_set[len(split_word_set)-1] ### for some word, it may contains / that is not for tagging mark\n",
    "\n",
    "def return_word_from_tagged_words(word):\n",
    "    # tagging_mark = \"/\"\n",
    "    split_word_set = word.split(\"/\")\n",
    "    return filter_alphanumeric(split_word_set[0])  ## For increasing speed\n",
    "#'/'.join(split_word_set[0:len(split_word_set)-1]) ### for some word, it may contains / that is not for tagging mark\n",
    "\n",
    "def return_Tagset_from_train_data(sentence_list):\n",
    "    total_Tagset=set({'^','$'})\n",
    "    for sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))## drop out empty items\n",
    "        for word in word_list:\n",
    "            tag = return_tag_from_words(word)\n",
    "            total_Tagset.add(tag)\n",
    "            total_Tagset = sorted(list(filter(None, total_Tagset)))\n",
    "    return dict(zip(total_Tagset, np.arange(0,len(total_Tagset),1)))\n",
    "#\n",
    "\n",
    "def return_Vocabulary_from_tagged_data(sentence_list):\n",
    "    total_vocabulary=set({'^','$'})\n",
    "    for sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))## drop out empty items\n",
    "        for word in word_list:\n",
    "            filtetered_word = return_word_from_tagged_words(word)\n",
    "            total_vocabulary.add(filtetered_word)\n",
    "    return dict(zip(sorted(list(total_vocabulary)), np.arange(0,len(total_vocabulary),1)))\n",
    "#sorted(list(total_vocabulary))\n",
    "\n",
    "def return_unigram_tag_counts(sentence_list, tag_set):\n",
    "    count_matrix =np.zeros((len(tag_set)))\n",
    "    #count_matrix += len(tag_set) ######## Add-one smoothing\n",
    "    ## Set value for sentence start '^' end '$'\n",
    "    count_matrix[tag_set['^']] += len(sentence_list)\n",
    "    count_matrix[tag_set['$']] += len(sentence_list)\n",
    "    for tem_sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", tem_sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))## drop out empty items\n",
    "        for word in word_list:\n",
    "            tag = return_tag_from_words(word)\n",
    "            if(tag in tag_set.keys()):\n",
    "                tag_index = tag_set[tag]\n",
    "                count_matrix[tag_index]+=1\n",
    "    return count_matrix\n",
    "\n",
    "def return_bigram_tag_counts(sentence_list, tag_set):\n",
    "    count_matrix=np.zeros((len(tag_set),len(tag_set)))\n",
    "    #count_matrix+=1  ######## Add-one smoothing\n",
    "    num_word=0\n",
    "    for tem_sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", tem_sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))## drop out empty items   \n",
    "        for i in range(0,len(word_list)-1):\n",
    "            num_word+=1\n",
    "            if (i==0):\n",
    "                first_tag = return_tag_from_words(word_list[0])\n",
    "                count_matrix[tag_set['^'], tag_set[first_tag]] += 1\n",
    "            else:\n",
    "                first_tag=return_tag_from_words(word_list[i])\n",
    "                second_tag=return_tag_from_words(word_list[i+1])\n",
    "            #if((first_word in vocabulary) & (second_word in vocabulary)):\n",
    "                first_digit=tag_set[first_tag]\n",
    "                second_digit=tag_set[second_tag]\n",
    "                count_matrix[first_digit,second_digit]+=1\n",
    "    print (\"Number of Words: \"+ str(num_word))\n",
    "    return count_matrix\n",
    "\n",
    "def return_bigram_tag2word_counts(sentence_list, tag_set, vocabulary):\n",
    "    count_matrix=np.zeros((len(tag_set),len(vocabulary)+1))\n",
    "    #count_matrix+=1  ######## Add-one smoothing\n",
    "    i=0\n",
    "    size_sentence=len(sentence_list)\n",
    "    for tem_sentence in sentence_list:\n",
    "        i+=1\n",
    "        if (i%10000==1):\n",
    "            print (\"Traning Complete Rate: \" + str(str(i/size_sentence)))\n",
    "        word_list = re.split(\"\\s+\", tem_sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))## drop out empty items        \n",
    "        for word in word_list:\n",
    "            tag = return_tag_from_words(word)\n",
    "            if(tag in tag_set):\n",
    "                filtered_word= return_word_from_tagged_words(word)\n",
    "                tag_index = tag_set[tag]\n",
    "                word_index = vocabulary[filtered_word] ### This process takes most of CPU time\n",
    "                count_matrix[tag_index, word_index]+=1\n",
    "            else:\n",
    "                continue\n",
    "    count_matrix[:,len(vocabulary)]+=1/len(vocabulary)\n",
    "    return count_matrix\n",
    "\n",
    "\n",
    "xx = return_sentence_list_from_file('brown.train.tagged.txt')\n",
    "\n",
    "sentence_list = xx#[0:10000]\n",
    "tag_dict = return_Tagset_from_train_data(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words: 964688\n",
      "Traning Complete Rate: 2.000040000800016e-05\n",
      "Traning Complete Rate: 0.2000240004800096\n",
      "Traning Complete Rate: 0.4000280005600112\n",
      "Traning Complete Rate: 0.6000320006400128\n",
      "Traning Complete Rate: 0.8000360007200144\n"
     ]
    }
   ],
   "source": [
    "xx = return_sentence_list_from_file('brown.train.tagged.txt')\n",
    "\n",
    "sentence_list = xx#[0:10000]\n",
    "tag_dict = return_Tagset_from_train_data(sentence_list)\n",
    "vocabulary_dict = return_Vocabulary_from_tagged_data(sentence_list)\n",
    "\n",
    "tag_unigram_counts = return_unigram_tag_counts(sentence_list, tag_dict)\n",
    "bigram_tag_counts = return_bigram_tag_counts(sentence_list, tag_dict)\n",
    "emission_counts = return_bigram_tag2word_counts(sentence_list, tag_dict, vocabulary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNAME=\"transition_matrix\"\\nSave_Transition_Matrix(probability_transition, NAME, tag_dict)\\nNAME=\"emission_matrix\"\\nSave_Emission_Matrix(probability_emission, NAME, tag_dict, vocabulary_dict)\\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_probability_transition(tag_unigram_counts, bigram_tag_counts):\n",
    "    size = len(tag_unigram_counts)\n",
    "    Probability_transition = np.zeros((size, size))\n",
    "    for i in range(len(tag_unigram_counts)):\n",
    "        Probability_transition[i] = bigram_tag_counts[i]/tag_unigram_counts[i]\n",
    "    return Probability_transition\n",
    "    \n",
    "def return_probability_emission(tag_unigram_counts, emission_counts):\n",
    "    matrix_nrow = len(tag_unigram_counts)\n",
    "    matrix_ncols = emission_counts.shape[1] ## number of columns+ last columns for Out of Vocabulary\n",
    "    Probability_emission = np.zeros((matrix_nrow, matrix_ncols))\n",
    "    for i in range(len(tag_unigram_counts)):\n",
    "        Probability_emission[i] = emission_counts[i]/tag_unigram_counts[i]\n",
    "    return Probability_emission\n",
    "\n",
    "def Save_Transition_Matrix(matrix, NAME, tag_set):\n",
    "    data_prob_output = matrix\n",
    "    file1 = open('Q2_HMM_'+ NAME + '_Model.txt',\"w\")\n",
    "\n",
    "    ## columns name\n",
    "    file1.writelines(NAME+\"\\t\")\n",
    "    file1.writelines('\\t'.join(map(str,list(tag_set))))\n",
    "    file1.writelines(\"\\n\")\n",
    "    i=1\n",
    "    for prob_row in data_prob_output:\n",
    "        file1.writelines([list(tag_dict)[i-1],'\\t'])\n",
    "        file1.writelines('\\t'.join(map(str,prob_row)))\n",
    "        file1.writelines(\"\\n\")\n",
    "        i+=1\n",
    "    file1.close()\n",
    "    print(\"Output Results can be found at the current directory!\")\n",
    "    print(\"Output Name is: \"+ 'Q2_HMM_'+ NAME + '_Model.txt')\n",
    "    return None\n",
    "\n",
    "def Save_Emission_Matrix(matrix, NAME, tag_set, vocabulary):\n",
    "    data_prob_output = matrix\n",
    "    file1 = open('Q2_a_'+ NAME + '_Model.txt',\"w\")\n",
    "\n",
    "    file1.writelines(NAME+\"\\t\")\n",
    "    file1.writelines('\\t'.join(map(str,list(vocabulary))))\n",
    "    file1.writelines('\\t'+\"<OOV>\"+\"\\n\")\n",
    "\n",
    "    i=1\n",
    "    for prob_row in data_prob_output:\n",
    "        file1.writelines([list(tag_dict)[i-1],'\\t'])\n",
    "        file1.writelines('\\t'.join(map(str,prob_row)))\n",
    "        file1.writelines(\"\\n\")\n",
    "        i+=1\n",
    "    file1.close()\n",
    "    print(\"Output Results can be found at the current directory!\")\n",
    "    print(\"Output Name is: \"+ 'Q2_HMM_'+ NAME + '_Model.txt')\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "probability_transition = return_probability_transition(tag_unigram_counts, bigram_tag_counts)\n",
    "\n",
    "probability_emission = return_probability_emission(tag_unigram_counts, emission_counts)\n",
    "\n",
    "'''\n",
    "NAME=\"transition_matrix\"\n",
    "Save_Transition_Matrix(probability_transition, NAME, tag_dict)\n",
    "NAME=\"emission_matrix\"\n",
    "Save_Emission_Matrix(probability_emission, NAME, tag_dict, vocabulary_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.68108222e-09],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.59040624e-10],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.99706348e-08],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.50997185e-07],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.68389603e-09],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.29515722e-05]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_transition\n",
    "probability_emission\n",
    "\n",
    "tag_dict\n",
    "vocabulary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.transpose(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_matrix_viterbi[i,j]\n",
    "list(tag_dict)[1]\n",
    "#vocabulary_dict\n",
    "\n",
    "tag_list = list(tag_dict)\n",
    "vocabulary_list= list(vocabulary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "       0.00000000e+00, 0.00000000e+00, 5.68108222e-09])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiation, for loop over all tags \n",
    "for i in range(len(tag_dict)):\n",
    "    ## from start state to first state\n",
    "    observation_index = vocabulary_dict[return_word_from_tagged_words(word_list[0])]\n",
    "    probability_matrix_viterbi[i,0] = probability_transition[tag_dict['^'],i] * probability_emission[i,observation_index]\n",
    "    max_index_tag = np.argmax(probability_matrix_viterbi[:,0]) ## Return max index from all tag\n",
    "    max_prob_tag = probability_matrix_viterbi[max_index_tag,0]\n",
    "    state_series.append(max_index_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probability_matrix_viterbi[:,0]\n",
    "tag_list[max_index]\n",
    "probability_matrix_viterbi[:,0].shape\n",
    "probability_matrix_viterbi[max_index_tag,0]\n",
    "#word_list[j]\n",
    "# Viterbi Algorithm\n",
    "Path_training_data= \"brown.train.tagged.txt\" #'brown.test.txt'\n",
    "head_file(Path_training_data, [0,1])\n",
    "sentence_list = return_sentence_list_from_file(Path_training_data)\n",
    "for sentence in sentence_list:\n",
    "    word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "    word_list = list(filter(None, word_list))## drop out empty items\n",
    "    break\n",
    "word_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For/in the/at central/jj storage/nn ,/, Tri-State/jj buys/vbz one/cd acre/nn ,/, Buries/vbz its/pp$ tanks/nns and/cc simply/rb holds/vbz permanent/jj title/nn to/in that/dt piece/nn ./. ',\n",
       " 'Enough/ap trading/vbg stamps/nns were/bed collected/vbn to/to buy/vb a/at 12-passenger/jj station/nn wagon/nn ./. ']"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_tags_from_HHM_with_viterbi(test_sentence_list, tag_dict, vocabulary_dict, probability_transition, probability_emission):\n",
    "    out_tag_list = []\n",
    "    tag_list=list(tag_dict)\n",
    "    for sentence in test_sentence_list:\n",
    "        word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))## drop out empty items\n",
    "        \n",
    "        ## Applying viterbi algorithm\n",
    "        probability_matrix_viterbi=np.zeros((len(tag_dict),len(word_list)))\n",
    "        for j in range(len(word_list)):\n",
    "            for i in range(len(tag_dict)): # for loop over all tags\n",
    "                filtered_word = return_word_from_tagged_words(word_list[j])\n",
    "                if (filtered_word in vocabulary_dict.keys()): \n",
    "                ## vocabulary_dict.keys() is much faster than list(vocabulary_dict)\n",
    "                    observation_index = vocabulary_dict[filtered_word]\n",
    "                else:\n",
    "                    ### Last Columns of emmision matrix is for <OOV>\n",
    "                    observation_index = len(vocabulary_dict) \n",
    "                    ### Last Columns of emmision matrix is for <OOV>\n",
    "                if (j==0):\n",
    "                    probability_matrix_viterbi[i,j] = probability_transition[tag_dict['^'],i] * probability_emission[i,observation_index]\n",
    "                else:\n",
    "                    probability_matrix_viterbi[i,j] = max_prob_tag*probability_transition[max_index_tag,i] * probability_emission[i,observation_index]\n",
    "\n",
    "            max_index_tag = np.argmax(probability_matrix_viterbi[:,j]) ## Return max index from all tag\n",
    "            max_prob_tag = probability_matrix_viterbi[max_index_tag,j]\n",
    "            \n",
    "            tag_label   = return_tag_from_words(word_list[j])\n",
    "            tag_from_HMM= tag_list[max_index_tag]\n",
    "            out_tag_list.append([tag_from_HMM,tag_label])\n",
    "    return out_tag_list\n",
    "\n",
    "Path_training_data= \"brown.test.tagged.txt\" #'brown.test.txt'\n",
    "#head_file(Path_training_data, [0,2])\n",
    "sentence_list = return_sentence_list_from_file(Path_training_data)\n",
    "predicted_tag_sequence = return_tags_from_HHM_with_viterbi(sentence_list[3:5], tag_dict, vocabulary_dict, probability_transition, probability_emission)\n",
    "sentence_list[3:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40033"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_dict['tristate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['in', 'in'], ['at', 'at'], ['jj', 'jj'], ['nn', 'nn'], ['.', ','], ['', 'jj'], ['', 'vbz'], ['', 'cd'], ['', 'nn'], ['', ','], ['', 'vbz'], ['', 'pp$'], ['', 'nns'], ['', 'cc'], ['', 'rb'], ['', 'vbz'], ['', 'jj'], ['', 'nn'], ['', 'in'], ['', 'dt'], ['', 'nn'], ['', '.'], ['ap', 'ap'], ['vbg', 'vbg'], ['nns', 'nns'], ['bed', 'bed'], ['vbn', 'vbn'], ['to', 'to'], ['vb', 'vb'], ['at', 'at'], ['nn', 'jj'], ['nn', 'nn'], ['nn', 'nn'], ['.', '.']]\n"
     ]
    }
   ],
   "source": [
    "print (predicted_tag_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4928541220538777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73419"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_size= len(predicted_tag_sequence)\n",
    "\n",
    "correct_count=0\n",
    "for x in predicted_tag_sequence:\n",
    "    #print (x)\n",
    "    if(x[0]==x[1]):\n",
    "        correct_count+=1\n",
    "print (correct_count/total_size)\n",
    "\n",
    "correct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probability_emission[tag_dict[''],:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_matrix_viterbi=np.zeros((len(tag_dict),len(word_list)))\n",
    "\n",
    "state_series = []\n",
    "state_sequence = []\n",
    "for j in range(len(word_list)):\n",
    "    for i in range(len(tag_dict)): # for loop over all tags\n",
    "        filtered_word = return_word_from_tagged_words(word_list[j])\n",
    "        if (filtered_word in vocabulary_dict.keys()):\n",
    "            observation_index = vocabulary_dict[filtered_word]\n",
    "        else:\n",
    "            observation_index = len(vocabulary_dict)+1 ### Last Columns of emmision matrix is for <OOV>\n",
    "        if (j==0):\n",
    "            probability_matrix_viterbi[i,j] = probability_transition[tag_dict['^'],i] * probability_emission[i,observation_index]\n",
    "        else:\n",
    "            probability_matrix_viterbi[i,j] = max_prob_tag*probability_transition[max_index_tag,i] * probability_emission[i,observation_index]\n",
    "    \n",
    "    max_index_tag = np.argmax(probability_matrix_viterbi[:,j]) ## Return max index from all tag\n",
    "    max_prob_tag = probability_matrix_viterbi[max_index_tag,j]\n",
    "    state_series.append(max_index_tag)\n",
    "    state_sequence.append(tag_list[max_index_tag])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tag_from_test_word(test_word, dict_tag, dict_vocabulary, probability_emission):\n",
    "    #this is baseline method\n",
    "    filtered_word = return_word_from_tagged_words(test_word)\n",
    "    \n",
    "    ## first locate word\n",
    "    if (filtered_word in dict_vocabulary.keys()):  ## vocabulary_dict.keys() is much faster than list(vocabulary_dict)\n",
    "        voc_index = dict_vocabulary[return_word_from_tagged_words(test_word)]\n",
    "    ## find tag with max p, \n",
    "        tag_max_P = list(dict_tag.keys())[np.argmax(probability_emission[:,voc_index])]\n",
    "    else:\n",
    "        tag_max_P='nn'\n",
    "    return tag_max_P \n",
    "\n",
    "def return_tag_model_testing(sentence_list, tag_set, vocabulary, probability_emission):\n",
    "    tag_list=[]\n",
    "    for sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))## drop out empty items\n",
    "        for word in word_list:\n",
    "            tag_model = return_tag_from_test_word(word, tag_set, vocabulary, probability_emission)\n",
    "            tag_label = return_tag_from_words(word)\n",
    "            tag_list.append([tag_model,tag_label])\n",
    "    return tag_list\n",
    "\n",
    "test_data_with_tag = return_sentence_list_from_file('brown.test.tagged.txt')\n",
    "test_data_tag_results = return_tag_model_testing(test_data_with_tag, tag_set, vocabulary, probability_emission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prob_output = probability_emission\n",
    "NAME=\"emission_matrix\"\n",
    "\n",
    "file1 = open('Q2_a_'+ NAME + '_Model.txt',\"w\")\n",
    "\n",
    "file1.writelines(NAME+\"\\t\")\n",
    "file1.writelines('\\t'.join(map(str,vocabulary)))\n",
    "file1.writelines(\"\\n\")\n",
    "\n",
    "i=1\n",
    "for prob_row in data_prob_output:\n",
    "    file1.writelines([tag_set[i-1],'\\t'])\n",
    "    file1.writelines('\\t'.join(map(str,prob_row)))\n",
    "    file1.writelines(\"\\n\")\n",
    "    i+=1\n",
    "file1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Results can be found at the current directory!\n",
      "Output Name is: Q2_HMM_transition_matrix_Model.txt\n"
     ]
    }
   ],
   "source": [
    "def Save_Emission_Matrix(matrix, NAME, tag_set, vocabulary):\n",
    "    data_prob_output = matrix\n",
    "    file1 = open('Q2_a_'+ NAME + '_Model.txt',\"w\")\n",
    "\n",
    "    file1.writelines(NAME+\"\\t\")\n",
    "    file1.writelines('\\t'.join(map(str,vocabulary)))\n",
    "    file1.writelines(\"\\n\")\n",
    "\n",
    "    i=1\n",
    "    for prob_row in data_prob_output:\n",
    "        file1.writelines([tag_set[i-1],'\\t'])\n",
    "        file1.writelines('\\t'.join(map(str,prob_row)))\n",
    "        file1.writelines(\"\\n\")\n",
    "        i+=1\n",
    "    file1.close()\n",
    "    print(\"Output Results can be found at the current directory!\")\n",
    "    print(\"Output Name is: \"+ 'Q2_HMM_'+ NAME + '_Model.txt')\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tag_from_test_word(test_word, tag_set, vocabulary, probability_emission):\n",
    "    #this is baseline method\n",
    "    filtered_word = return_word_from_tagged_words(test_word)\n",
    "    ## first locate word\n",
    "    if (filtered_word in vocabulary):\n",
    "        voc_index = vocabulary[return_word_from_tagged_words(test_word)]\n",
    "    ## find tag with max p, \n",
    "        tag_max_P = tag_set[np.argmax(probability_emission[:,voc_index])]\n",
    "    else:\n",
    "        tag_max_P='nn'\n",
    "    return tag_max_P \n",
    "\n",
    "def return_tag_model_testing(sentence_list, tag_set, vocabulary, probability_emission):\n",
    "    tag_list=[]\n",
    "    for sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))## drop out empty items\n",
    "        for word in word_list:\n",
    "            tag_model = return_tag_from_test_word(word, tag_set, vocabulary, probability_emission)\n",
    "            tag_label = return_tag_from_words(word)\n",
    "            tag_list.append([tag_model,tag_label])\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148968\n",
      "Output Results can be found at the current directory!\n",
      "Output Name is: Q2_a_test_out_Model.txt\n"
     ]
    }
   ],
   "source": [
    "def Save_Results(matrix, NAME):\n",
    "    data_prob_output = matrix\n",
    "    file1 = open('Q2_a_'+ NAME + '_Model.txt',\"w+\")\n",
    "\n",
    "    file1.writelines(['predicted_tag', '\\t', 'real_tag', '\\n'])\n",
    "    i=1\n",
    "    for prob_row in data_prob_output:\n",
    "        file1.writelines([prob_row[0],\"\\t\", prob_row[1], \"\\n\"]) \n",
    "        i+=1\n",
    "    file1.close()\n",
    "    print (i)\n",
    "    print(\"Output Results can be found at the current directory!\")\n",
    "    print(\"Output Name is: \"+ 'Q2_a_'+ NAME + '_Model.txt')\n",
    "    return None\n",
    "\n",
    "Save_Results(test_data_tag_results, 'test_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=test_data_tag_results, columns=['tag_from_baseline','tag_from_given_label']).to_csv('Q2_a_Baseline_Results.txt', sep=\"\\t\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108818"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7304839326830774\n"
     ]
    }
   ],
   "source": [
    "total_size= len(test_data_tag_results)\n",
    "\n",
    "correct_count=0\n",
    "for x in test_data_tag_results:\n",
    "    #print (x)\n",
    "    if(x[0]==x[1]):\n",
    "        correct_count+=1\n",
    "print (correct_count/total_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '$', \"'\", \"''\", '(', ')', '*', ',', '--', '.', ':', '^', '``',\n",
       "       'abl', 'abn', 'abx', 'ann', 'anp', 'ap', 'ap$', 'ap+ap', 'at',\n",
       "       'be', 'bed', 'bedz', 'beg', 'bem', 'ben', 'ber', 'ber-n', 'bez',\n",
       "       'cc', 'cd', 'cd$', 'cs', 'dbez', 'dmd', 'do', 'dod', 'doz', 'dt',\n",
       "       'dt$', 'dti', 'dts', 'dtx', 'ex', 'hv', 'hv+to', 'hvd', 'hvg',\n",
       "       'hvn', 'hvz', 'in', 'in+at', 'in+at-t', 'jj', 'jj$', 'jj+jj',\n",
       "       'jjr', 'jjs', 'jjt', 'md', 'md+hv', 'md+ppss', 'md+to', 'nil',\n",
       "       'nn', 'nn$', 'nn+in', 'nn+nn', 'nns', 'nns$', 'np', 'np$', 'nps',\n",
       "       'nps$', 'nr', 'nr$', 'nrs', 'od', 'pn', 'pn$', 'pp$', 'pp$$',\n",
       "       'ppl', 'ppls', 'ppo', 'ppo+in', 'pps', 'ppss', 'ppss+bem', 'ql',\n",
       "       'qlp', 'rb', 'rb$', 'rb+cc', 'rbr', 'rbt', 'rn', 'rp', 'rp+in',\n",
       "       'to', 'uh', 'vb', 'vb+at', 'vb+in', 'vb+jj', 'vb+to', 'vb+vb',\n",
       "       'vbd', 'vbg', 'vbg+to', 'vbn', 'vbn+to', 'vbz', 'wdber',\n",
       "       'wdber+pp', 'wdbez', 'wddo+pps', 'wddod', 'wdhvz', 'wdt', 'wp$',\n",
       "       'wpo', 'wps', 'wql', 'wrb', 'wrb+do'], dtype=object)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unigram = pd.DataFrame(data=tag_unigram_counts, index=tag_set).T\n",
    "df_unigram.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148967"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data_tag_labels)\n",
    "#len(test_data_tag_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ber-n'"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word = \"re\"\n",
    "return_tag_from_test_word(test_word, tag_set, vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = return_sentence_list_from_file('brown.test.txt')\n",
    "#test_word = return_word_list_from_file('brown.test.txt')\n",
    "test_word_with_tag = return_sentence_list_from_file('brown.test.tagged.txt')\n",
    "\n",
    "\n",
    "for sentence in test_data:\n",
    "    word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "    word_list = list(filter(None, word_list))## drop out empty items  \n",
    "    for word in word_list:\n",
    "            tag = return_tag_from_test_word(word)\n",
    "            total_Tagset.add(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"``/`` He/pps almost/rb brought/vbd it/ppo back/rb all/abn the/at way/nn ''/'' ./. \", 'There/ex is/bez a/at vast/jj difference/nn between/in the/at community/nn of/in reconciliation/nn which/wdt the/at New/jj Testament/nn describes/vbz and/cc the/at community/nn of/in congeniality/nn found/vbn in/in the/at average/jj church/nn building/nn ./. ']\n"
     ]
    }
   ],
   "source": [
    "print (sentence_list)\n",
    "## make a dictionary\n",
    "dict_tag = dict(zip(tag_set, tag_counts))\n",
    "#dict_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"``/`` He/pps almost/rb brought/vbd it/ppo back/rb all/abn the/at way/nn ''/'' ./. \"]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'``'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_mark=\"/\"\n",
    "for sentence in xx[0:1]:\n",
    "    word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "    for word in word_list:\n",
    "        if(word!=''):\n",
    "            filtered_word = word.split(tagging_mark)[0]\n",
    "            tag = word.split(tagging_mark)[1]\n",
    "#re.split(\"\\\" , word_list[0] )\n",
    "\n",
    "word_list[0].split(\"/\")[0]\n",
    "word_list[0].split(\"/\")[1]\n",
    "#return_vocabulary_from_sentence_list(xx[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_Probability_of_Sentence_Word(sentence, norm_bigram, vocabulary, p_unseen):\n",
    "    word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "    Log_Probability_sentence = 0 \n",
    "    ## Filter Null Element\n",
    "    word_list = list(filter(None, word_list))\n",
    "    \n",
    "    for i in range(0,len(word_list)-1):\n",
    "        if (i==0):\n",
    "            first_word=filter_alphanumeric(word_list[0])\n",
    "            if(first_word in vocabulary):\n",
    "                Log_Probability_sentence += np.log(norm_bigram[vocabulary.index('^'), vocabulary.index(first_word)])\n",
    "            else:\n",
    "                ## assuming new word as a probability of 1/V\n",
    "                ## Good Turing Smoothing for zero count\n",
    "                Log_Probability_sentence += np.log(p_unseen) # np.log(1/len(vocabulary)**2) #\n",
    "        tem_bigram_count_pairs = word_list[i:i+2]\n",
    "        first_word=filter_alphanumeric(tem_bigram_count_pairs[0])\n",
    "        second_word=filter_alphanumeric(tem_bigram_count_pairs[1])\n",
    "        if((first_word in vocabulary) & (second_word in vocabulary)):\n",
    "            first_index=vocabulary.index(first_word)\n",
    "            second_index=vocabulary.index(second_word)\n",
    "            prob_word_pair=norm_bigram[first_index, second_index]\n",
    "            if (prob_word_pair!=0):\n",
    "                Log_Probability_sentence += np.log(prob_word_pair)\n",
    "            else:\n",
    "                Log_Probability_sentence += np.log(p_unseen)\n",
    "            #print (first_word + \" \"+ second_word )\n",
    "            #print (Log_Probability_sentence)\n",
    "        else:\n",
    "            ## assuming new word as a probability of 1/V\n",
    "            Log_Probability_sentence += np.log(p_unseen) #np.log(1/len(vocabulary)**2)\n",
    "    return np.e**Log_Probability_sentence\n",
    "\n",
    "def return_Probability_of_Sentence_trigram_Word(sentence, norm_bigram, vocabulary, p_unseen):\n",
    "    word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "    Log_Probability_sentence = 0 \n",
    "    ## Filter Null Element\n",
    "    word_list = list(filter(None, word_list))\n",
    "    \n",
    "    for i in range(0,len(word_list)-2):\n",
    "        if (i==0):\n",
    "            first_word=filter_alphanumeric(word_list[0])\n",
    "            second_word=filter_alphanumeric(word_list[1])\n",
    "            if ((first_word in vocabulary) & (second_word in vocabulary)):\n",
    "                Log_Probability_sentence += np.log(norm_bigram[vocabulary.index('^'), vocabulary.index(first_word)])\n",
    "                Log_Probability_sentence += np.log(norm_bigram[vocabulary.index(first_word),vocabulary.index(second_word)])\n",
    "            else:\n",
    "                Log_Probability_sentence += 2*np.log(p_unseen) # np.log(1/len(vocabulary)**2) #\n",
    "        tem_trigram_count_pairs = word_list[i:i+3]\n",
    "        first_word=filter_alphanumeric(tem_trigram_count_pairs[0])\n",
    "        second_word=filter_alphanumeric(tem_trigram_count_pairs[1])\n",
    "        third_word=filter_alphanumeric(tem_trigram_count_pairs[2])\n",
    "        if((first_word in vocabulary) & (second_word in vocabulary) & (third_word in vocabulary)):\n",
    "            first_index=vocabulary.index(first_word)\n",
    "            second_index=vocabulary.index(second_word)\n",
    "            third_index=vocabulary.index(third_word)                                                  \n",
    "            Log_Probability_sentence += np.log(norm_bigram[first_index,second_index])    \n",
    "            Log_Probability_sentence += np.log(norm_bigram[second_index,third_index])       \n",
    "        else:\n",
    "            Log_Probability_sentence += 2*np.log(p_unseen) #np.log(1/len(vocabulary)**2)\n",
    "    return np.e**Log_Probability_sentence\n",
    "\n",
    "\n",
    "def return_probability(Path_Test_Data, norm_bigram, vocabulary):\n",
    "    count_matrix=list()\n",
    "    P_unseen=norm_bigram.min()\n",
    "    with open(Path_Test_Data,  mode='r', newline='') as file:\n",
    "        i=0\n",
    "        for line in file:\n",
    "            results= return_Probability_of_Sentence_trigram_Word(line, norm_bigram, vocabulary, P_unseen)\n",
    "            count_matrix.append(results)\n",
    "            i+=1\n",
    "    return count_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"``/`` He/pps almost/rb brought/vbd it/ppo back/rb all/abn the/at way/nn ''/'' ./. \""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Main\n",
    "def test_with_word_model(path_train_data, path_test_data):  \n",
    "    ## first generate sentence list\n",
    "    sentence_list = return_sentence_list_from_file(path_train_data)\n",
    "    ## building vocabulary\n",
    "    vocabulary = return_vocabulary_from_sentence_list(sentence_list)\n",
    "\n",
    "    raw_unigram_word_counts = return_unigram_counts(sentence_list, vocabulary)\n",
    "    raw_bigram_word_counts  = return_bigram_word_counts(sentence_list,vocabulary)   \n",
    "    norm_bigram_word_counts = Norm_Bigram(raw_unigram_word_counts, raw_bigram_word_counts, vocabulary)\n",
    "    \n",
    "    # test data output\n",
    "    prob = return_probability(path_test_data, norm_bigram_word_counts, vocabulary)\n",
    "    return prob\n",
    "\n",
    "def Save_Results(En_prob,FR_prob,GR_prob, NAME):\n",
    "    data_prob_output = np.transpose([En_prob,FR_prob,GR_prob])\n",
    "    index_LANG=['EN','FR','GR']\n",
    "    #out_index=list()\n",
    "    file1 = open('Results_'+ NAME + '_Model.txt',\"w\")\n",
    "    file1.writelines(['ID','LANG'])\n",
    "    i=1\n",
    "    for prob_row in data_prob_output[:]:\n",
    "        index_l = list(prob_row).index(max(prob_row))\n",
    "        #out_index.append([i,index_LANG[index_l]])\n",
    "        file1.writelines([str(i),index_LANG[index_l]]) \n",
    "        i+=1\n",
    "    file1.close() #to change file access modes\n",
    "    \n",
    "    #pd.DataFrame(out_index,columns=['ID','LANG']).set_index('ID').to_csv('Results_'+ NAME + '_Model.txt', sep=\"\\t\")\n",
    "    print(\"Output Results can be found at the current directory!\")\n",
    "    print(\"Output Name is: \"+ 'Results_'+ NAME + '_Model.txt')\n",
    "    return data_prob_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file1 = open(\"myfile.txt\",\"w\") \n",
    "L = [\"ID\\t\",\"LANG\\n\"]  \n",
    "  \n",
    "# \\n is placed to indicate EOL (End of Line) \n",
    "\n",
    "file1.writelines([\"a\\t\",'b']) \n",
    "file1.close() #to change file access modes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words: 30142\n",
      "Number of Words: 34062\n",
      "Number of Words: 26913\n",
      "Output Results can be found at the current directory!\n",
      "Output Name is: Results_Word_Trigram_Model.txt\n"
     ]
    }
   ],
   "source": [
    "Path_Data='Data/'\n",
    "Train_Data_Set=['EN.txt', 'FR.txt', 'GR.txt']\n",
    "Path_test_data = Path_Data+'LangID.test.txt'\n",
    "All_prob=list()\n",
    "for name_train in Train_Data_Set:\n",
    "    Path_train_data = Path_Data+name_train\n",
    "    All_prob.append(test_with_word_model(Path_train_data, Path_test_data))\n",
    "\n",
    "\n",
    "NAME='Word_Trigram'\n",
    "output = Save_Results(All_prob[0],All_prob[1],All_prob[2], NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Path_Data='Data/'\n",
    "Train_Data_Set=['EN.txt', 'FR.txt', 'GR.txt']\n",
    "Path_Train_Data=Path_Data+Train_Data_Set[0]\n",
    "Path_test_data = Path_Data+'LangID.test.txt'\n",
    "\n",
    "sentence_list=return_sentence_list_from_file(Path_Train_Data)\n",
    "vocabulary = return_vocabulary_from_sentence_list(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words: 30142\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (4402, 4402, 4402) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1ab585497c85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mraw_unigram_word_counts\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mreturn_unigram_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mraw_bigram_word_counts\u001b[0m  \u001b[0;34m=\u001b[0m\u001b[0mreturn_bigram_word_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mraw_trigram_word_counts\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mreturn_Trigram_word_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#norm_bigram_word_counts = Norm_Bigram(raw_unigram_word_counts, raw_bigram_word_counts, vocabulary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-828dec662344>\u001b[0m in \u001b[0;36mreturn_Trigram_word_counts\u001b[0;34m(sentence_list, vocabulary)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreturn_Trigram_word_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mcount_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;31m#count_matrix+=1  ######## Add-one smoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mnum_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (4402, 4402, 4402) and data type float64"
     ]
    }
   ],
   "source": [
    "Path_Data='Data/'\n",
    "Train_Data_Set=['EN.txt', 'FR.txt', 'GR.txt']\n",
    "Path_Train_Data=Path_Data+Train_Data_Set[0]\n",
    "Path_test_data = Path_Data+'LangID.test.txt'\n",
    "\n",
    "sentence_list=return_sentence_list_from_file(Path_Train_Data)\n",
    "vocabulary = return_vocabulary_from_sentence_list(sentence_list)\n",
    "\n",
    "raw_unigram_word_counts =return_unigram_counts(sentence_list,vocabulary)\n",
    "raw_bigram_word_counts  =return_bigram_word_counts(sentence_list,vocabulary)\n",
    "raw_trigram_word_counts =return_Trigram_word_counts(sentence_list,vocabulary)\n",
    "#norm_bigram_word_counts = Norm_Bigram(raw_unigram_word_counts, raw_bigram_word_counts, vocabulary)\n",
    "\n",
    "norm_trigram_word_counts = Norm_Trigram(raw_unigram_word_counts, raw_bigram_word_counts, raw_trigram_word_counts, vocabulary)\n",
    "\n",
    "#Return_c_from_Good_Turing_Smoothing(raw_unigram_word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EN.txt', 'FR.txt', 'GR.txt', 'LangID.gold.txt', 'LangID.test.txt', 'Letter_Bigram_Model.py', 'Results_Letter_Bigram_Model.txt', 'Results_Word_Bigram_Good_Turing_Smoothing_Model.txt', 'Results_Word_Bigram_Model.txt', 'Results_Word_Trigram_Model.txt', 'run_all_python_scripts_for_HW1.sh', 'Word_Bigram_Model.py', 'Word_Bigram_Model_bp.py', 'Word_Bigram_Model_Good_Turing_Smoothing.py', 'Word_Trigram_Model.py']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LANG_ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID LANG_ref\n",
       "0  1.0       EN\n",
       "1  2.0       EN\n",
       "2  3.0       EN\n",
       "3  4.0       EN\n",
       "4  5.0       EN"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path='Data/'\n",
    "xx = os.listdir(Path)\n",
    "print (xx)\n",
    "df_letter = pd.read_csv(Path+'Results_Letter_Bigram_Model.txt', sep=\"\\t\").rename(columns={'LANG':'LANG_letter'})\n",
    "df_Word = pd.read_csv(Path+'Results_Word_Bigram_Model.txt', sep=\"\\t\").rename(columns={'LANG':'LANG_Word'})\n",
    "df_Word_Good = pd.read_csv(Path+'Results_Word_Bigram_Good_Turing_Smoothing_Model.txt', sep=\"\\t\").rename(columns={'LANG':'LANG_Word_Good_Turing'})\n",
    "df_reference = pd.read_csv(Path+'LangID.gold.txt', sep=\" \").rename(columns={'LANG':'LANG_ref'})\n",
    "df_trigram = pd.read_csv(Path+'Results_Word_Trigram_Model.txt', sep=\"\\t\").rename(columns={'LANG':'LANG_Trigram'})\n",
    "df_reference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LANG_ref</th>\n",
       "      <th>LANG_Word</th>\n",
       "      <th>LANG_Word_Good_Turing</th>\n",
       "      <th>LANG_letter</th>\n",
       "      <th>LANG_Trigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>GR</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>GR</td>\n",
       "      <td>GR</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID LANG_ref LANG_Word LANG_Word_Good_Turing LANG_letter LANG_Trigram\n",
       "0  1.0       EN        EN                    EN          EN           EN\n",
       "1  2.0       EN        EN                    GR          EN           EN\n",
       "2  3.0       EN        EN                    GR          GR           EN\n",
       "3  4.0       EN        EN                    EN          EN           EN\n",
       "4  5.0       EN        EN                    EN          EN           EN"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum = df_reference.merge(df_Word.merge(df_Word_Good.merge(df_letter.merge(df_trigram, on='ID'), on='ID'), on='ID'), on='ID')\n",
    "df_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8666666666666667\n",
      "0.9733333333333334\n",
      "0.92\n",
      "0.7733333333333333\n"
     ]
    }
   ],
   "source": [
    "print (len(df_sum[df_sum['LANG_ref']==df_sum['LANG_letter']])/150.0)\n",
    "\n",
    "print (len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Word']])/150.0)\n",
    "\n",
    "print (len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Word_Good_Turing']])/150.0)\n",
    "\n",
    "print (len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Trigram']])/150.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (4402, 4402, 4402) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-49f10290b624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#4402**3/1024**3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4402\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4402\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4402\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (4402, 4402, 4402) and data type float64"
     ]
    }
   ],
   "source": [
    "#4402**3/1024**3\n",
    "np.zeros((4402,4402,4402))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accuracy=[1.0, len(df_sum[df_sum['LANG_ref']==df_sum['LANG_letter']])/150.0, len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Word']])/150.0,\n",
    "        len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Word_Good_Turing']])/150.0, len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Trigram']])/150.0]\n",
    "Accuracy\n",
    "#df.append({'foo':1, 'bar':2}, ignore_index=True)\n",
    "df_sum.set_index('ID').append({'LANG_ref':1.0, 'LANG_Word':len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Word']])/150.0 , \n",
    "                               'LANG_Word_Good_Turing':len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Word_Good_Turing']])/150.0 ,\n",
    "                               'LANG_Trigram':len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Trigram']])/150.0,\n",
    "                               'LANG_letter':len(df_sum[df_sum['LANG_ref']==df_sum['LANG_letter']])/150.0 \n",
    "                              },ignore_index=True).to_csv('All_Models_Summary.txt', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
