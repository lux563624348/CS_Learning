{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def head_file(file_path, num_rows):\n",
    "    ## num_rows has to be a 2 elements list\n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        row_range=range(num_rows[0],num_rows[1]+1)\n",
    "        i=0\n",
    "        for line in file:\n",
    "            if (i in row_range):               \n",
    "                   print (line)\n",
    "            i+=1\n",
    "    return None\n",
    "\n",
    "def filter_alphanumeric(word):\n",
    "#\\w matches any alphanumeric character\n",
    "    merge_words_no_digit=''\n",
    "    if (word!=''):\n",
    "        all_match = re.findall('\\w+', word)\n",
    "        all_match = list(filter(None, all_match))\n",
    "        merge_words=''\n",
    "        for item in all_match:\n",
    "            merge_words+=item\n",
    "        ## Find All unicode, then all non digits\n",
    "        merge_words=re.findall('[^(\\_|\\d)]', merge_words)\n",
    "        for item in merge_words:\n",
    "            merge_words_no_digit+=item        \n",
    "    else:\n",
    "        #print (\"Warning, One word is empty.\")\n",
    "        return '' \n",
    "    return merge_words_no_digit.lower()\n",
    "\n",
    "def return_word_list_from_file(Path_File):\n",
    "    list_words=list()\n",
    "    with open(Path_File,  mode='r', newline='') as file:\n",
    "        for line in file:\n",
    "            for word in (re.split(\"\\s+\", line.rstrip('\\n'))):\n",
    "                if (word !=''):\n",
    "                    list_words.append(filter_alphanumeric(word))\n",
    "## if different languague, above line has to be changed\n",
    "    list_words = list(filter(None, list_words))\n",
    "    return list_words\n",
    "\n",
    "\n",
    "def return_sentence_list_from_file(Path_File):\n",
    "    list_sentences=list()\n",
    "    with open(Path_File,  mode='r', newline='') as file:\n",
    "        for line in file:\n",
    "            if (len(line)>1):\n",
    "                sentence = line.rstrip('\\n').rstrip('\\r')\n",
    "                #marked_sentence = #'<^> '+line.rstrip('\\n').rstrip('\\r')+' </s>'\n",
    "                list_sentences.append(sentence)\n",
    "    list_sentences = list(filter(None, list_sentences))\n",
    "    return list_sentences\n",
    "\n",
    "def return_vocabulary_from_sentence_list(sentence_list):\n",
    "    total_vocabulary=set({'^','$'})\n",
    "    for sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "        for word in word_list:\n",
    "            filtered_word = filter_alphanumeric(word)\n",
    "            if(filtered_word!=''):\n",
    "                total_vocabulary.add(filtered_word)\n",
    "    return sorted(list(total_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_unigram_counts(sentence_list, vocabulary):\n",
    "    count_matrix =np.zeros((len(vocabulary)))\n",
    "    count_matrix += len(vocabulary) ######## Add-one smoothing\n",
    "    ## Set value for sentence start <s>\n",
    "    count_matrix[vocabulary.index('^')] += len(sentence_list)\n",
    "    count_matrix[vocabulary.index('$')] += len(sentence_list)\n",
    "    for tem_sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", tem_sentence.rstrip('\\n'))\n",
    "        for word in word_list:\n",
    "            filtered_word = filter_alphanumeric(word)\n",
    "            if(filtered_word in vocabulary):\n",
    "                word_index = vocabulary.index(filtered_word)\n",
    "                count_matrix[word_index]+=1\n",
    "    return count_matrix\n",
    "\n",
    "def Return_c_from_Good_Turing_Smoothing(unigram_count_matrix):\n",
    "    N_1=0\n",
    "    N_2=0\n",
    "    N_words = unigram_count_matrix.sum()\n",
    "    for x in unigram_count_matrix:\n",
    "        if(x==1):\n",
    "            N_1+=1\n",
    "        elif(x==2):\n",
    "            N_2+=1\n",
    "        else:\n",
    "            continue\n",
    "    c_for_zero = 1.0*N_1/N_words\n",
    "    c_for_once  = 1.0*(1+1)*(N_2/N_1)\n",
    "    return c_for_zero\n",
    "\n",
    "\n",
    "def return_bigram_word_counts(sentence_list, vocabulary):\n",
    "    count_matrix=np.zeros((len(vocabulary),len(vocabulary)))\n",
    "    #count_matrix+=1  ######## Add-one smoothing\n",
    "    num_word=0\n",
    "    for tem_sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", tem_sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))\n",
    "        \n",
    "        for i in range(0,len(word_list)-1):\n",
    "            num_word+=1\n",
    "            if (i==0):\n",
    "                first_word = filter_alphanumeric(word_list[0])\n",
    "                if(first_word in vocabulary):\n",
    "                    count_matrix[vocabulary.index('^'), vocabulary.index(first_word)] += 1\n",
    "            tem_bigram_count_pairs = word_list[i:i+2]\n",
    "            first_word=filter_alphanumeric(tem_bigram_count_pairs[0])\n",
    "            second_word=filter_alphanumeric(tem_bigram_count_pairs[1])\n",
    "            \n",
    "            if((first_word in vocabulary) & (second_word in vocabulary)):\n",
    "                first_digit=vocabulary.index(first_word)\n",
    "                second_digit=vocabulary.index(second_word)\n",
    "                count_matrix[first_digit,second_digit]+=1\n",
    "    print (\"Number of Words: \"+ str(num_word))\n",
    "    return count_matrix\n",
    "\n",
    "def return_Trigram_word_counts(sentence_list, vocabulary):\n",
    "    count_matrix=np.zeros((len(vocabulary),len(vocabulary)))\n",
    "    #count_matrix+=1  ######## Add-one smoothing\n",
    "    num_word=0\n",
    "    for tem_sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", tem_sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))\n",
    "        \n",
    "        for i in range(0,len(word_list)-2):\n",
    "            num_word+=1\n",
    "            if (i==0):\n",
    "                first_word = filter_alphanumeric(word_list[0])\n",
    "                second_word = filter_alphanumeric(word_list[1])\n",
    "                if((first_word in vocabulary) & ((second_word in vocabulary))):\n",
    "                    count_matrix[vocabulary.index(first_word), vocabulary.index(second_word)] += 1\n",
    "            tem_trigram_count_chain = word_list[i:i+3]\n",
    "            pre_two_words=tem_trigram_count_chain[0:2]\n",
    "            \n",
    "            third_word=filter_alphanumeric(tem_trigram_count_chain[2])\n",
    "            if((third_word in vocabulary)):\n",
    "                first_digit=vocabulary.index(pre_two_words)\n",
    "                second_digit=vocabulary.index(third_word)\n",
    "                count_matrix[first_digit,second_digit]+=1\n",
    "    print (\"Number of Words: \"+ str(num_word))\n",
    "    return count_matrix\n",
    "\n",
    "\n",
    "def Norm_Bigram(unigram_counts, bigram_counts, vocabulary):\n",
    "    #c_0 = Return_c_from_Good_Turing_Smoothing(unigram_counts)\n",
    "    c_0=1.0\n",
    "    norm_bigram = np.zeros((len(vocabulary),len(vocabulary))) \n",
    "    for i in range(len(vocabulary)):\n",
    "        norm_bigram[i,:]=(bigram_counts[i,:]+c_0)/(unigram_counts[i])\n",
    "    return norm_bigram\n",
    "\n",
    "def Norm_Trigram(unigram_counts, bigram_counts, trigram_counts, vocabulary):\n",
    "    #c_0 = Return_c_from_Good_Turing_Smoothing(unigram_counts)\n",
    "    norm_trigram = np.zeros((len(vocabulary),len(vocabulary)))\n",
    "    for i in range(len(vocabulary)):\n",
    "            norm_trigram[i,:]=(norm_trigram[i,:]+c_0)/(bigram_counts[i])\n",
    "    return norm_trim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_Probability_of_Sentence_Word(sentence, norm_bigram, vocabulary, p_unseen):\n",
    "    word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "    Log_Probability_sentence = 0 \n",
    "    ## Filter Null Element\n",
    "    word_list = list(filter(None, word_list))\n",
    "    \n",
    "    for i in range(0,len(word_list)-1):\n",
    "        if (i==0):\n",
    "            first_word=filter_alphanumeric(word_list[0])\n",
    "            if(first_word in vocabulary):\n",
    "                Log_Probability_sentence += np.log(norm_bigram[vocabulary.index('^'), vocabulary.index(first_word)])\n",
    "            else:\n",
    "                ## assuming new word as a probability of 1/V\n",
    "                ## Good Turing Smoothing for zero count\n",
    "                Log_Probability_sentence += np.log(p_unseen) # np.log(1/len(vocabulary)**2) #\n",
    "        tem_bigram_count_pairs = word_list[i:i+2]\n",
    "        first_word=filter_alphanumeric(tem_bigram_count_pairs[0])\n",
    "        second_word=filter_alphanumeric(tem_bigram_count_pairs[1])\n",
    "        if((first_word in vocabulary) & (second_word in vocabulary)):\n",
    "            first_index=vocabulary.index(first_word)\n",
    "            second_index=vocabulary.index(second_word)\n",
    "            prob_word_pair=norm_bigram[first_index, second_index]\n",
    "            if (prob_word_pair!=0):\n",
    "                Log_Probability_sentence += np.log(prob_word_pair)\n",
    "            else:\n",
    "                Log_Probability_sentence += np.log(p_unseen)\n",
    "            #print (first_word + \" \"+ second_word )\n",
    "            #print (Log_Probability_sentence)\n",
    "        else:\n",
    "            ## assuming new word as a probability of 1/V\n",
    "            Log_Probability_sentence += np.log(p_unseen) #np.log(1/len(vocabulary)**2)\n",
    "    return np.e**Log_Probability_sentence\n",
    "\n",
    "def return_Probability_of_Sentence_trigram_Word(sentence, norm_bigram, vocabulary, p_unseen):\n",
    "    word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "    Log_Probability_sentence = 0 \n",
    "    ## Filter Null Element\n",
    "    word_list = list(filter(None, word_list))\n",
    "    \n",
    "    for i in range(0,len(word_list)-2):\n",
    "        if (i==0):\n",
    "            first_word=filter_alphanumeric(word_list[0])\n",
    "            second_word=filter_alphanumeric(word_list[1])\n",
    "            if ((first_word in vocabulary) & (second_word in vocabulary)):\n",
    "                Log_Probability_sentence += np.log(norm_bigram[vocabulary.index('^'), vocabulary.index(first_word)])\n",
    "                Log_Probability_sentence += np.log(norm_bigram[vocabulary.index(first_word),vocabulary.index(second_word)])\n",
    "            else:\n",
    "                Log_Probability_sentence += 2*np.log(p_unseen) # np.log(1/len(vocabulary)**2) #\n",
    "        tem_trigram_count_pairs = word_list[i:i+3]\n",
    "        first_word=filter_alphanumeric(tem_trigram_count_pairs[0])\n",
    "        second_word=filter_alphanumeric(tem_trigram_count_pairs[1])\n",
    "        third_word=filter_alphanumeric(tem_trigram_count_pairs[2])\n",
    "        if((first_word in vocabulary) & (second_word in vocabulary) & (third_word in vocabulary)):\n",
    "            first_index=vocabulary.index(first_word)\n",
    "            second_index=vocabulary.index(second_word)\n",
    "            third_index=vocabulary.index(third_word)                                                  \n",
    "            Log_Probability_sentence += np.log(norm_bigram[first_index,second_index])    \n",
    "            Log_Probability_sentence += np.log(norm_bigram[second_index,third_index])       \n",
    "        else:\n",
    "            Log_Probability_sentence += 2*np.log(p_unseen) #np.log(1/len(vocabulary)**2)\n",
    "    return np.e**Log_Probability_sentence\n",
    "\n",
    "\n",
    "def return_probability(Path_Test_Data, norm_bigram, vocabulary):\n",
    "    count_matrix=list()\n",
    "    P_unseen=norm_bigram.min()\n",
    "    with open(Path_Test_Data,  mode='r', newline='') as file:\n",
    "        i=0\n",
    "        for line in file:\n",
    "            results= return_Probability_of_Sentence_trigram_Word(line, norm_bigram, vocabulary, P_unseen)\n",
    "            count_matrix.append(results)\n",
    "            i+=1\n",
    "    return count_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Main\n",
    "def test_with_word_model(path_train_data, path_test_data):  \n",
    "    ## first generate sentence list\n",
    "    sentence_list = return_sentence_list_from_file(path_train_data)\n",
    "    ## building vocabulary\n",
    "    vocabulary = return_vocabulary_from_sentence_list(sentence_list)\n",
    "\n",
    "    raw_unigram_word_counts = return_unigram_counts(sentence_list, vocabulary)\n",
    "    raw_bigram_word_counts  = return_bigram_word_counts(sentence_list,vocabulary)   \n",
    "    norm_bigram_word_counts = Norm_Bigram(raw_unigram_word_counts, raw_bigram_word_counts, vocabulary)\n",
    "    \n",
    "    # test data output\n",
    "    prob = return_probability(path_test_data, norm_bigram_word_counts, vocabulary)\n",
    "    return prob\n",
    "\n",
    "def Save_Results(En_prob,FR_prob,GR_prob, NAME):\n",
    "    data_prob_output = np.transpose([En_prob,FR_prob,GR_prob])\n",
    "    index_LANG=['EN','FR','GR']\n",
    "    #out_index=list()\n",
    "    file1 = open('Results_'+ NAME + '_Model.txt',\"w\")\n",
    "    file1.writelines(['ID','LANG'])\n",
    "    i=1\n",
    "    for prob_row in data_prob_output[:]:\n",
    "        index_l = list(prob_row).index(max(prob_row))\n",
    "        #out_index.append([i,index_LANG[index_l]])\n",
    "        file1.writelines([str(i),index_LANG[index_l]]) \n",
    "        i+=1\n",
    "    file1.close() #to change file access modes\n",
    "    \n",
    "    #pd.DataFrame(out_index,columns=['ID','LANG']).set_index('ID').to_csv('Results_'+ NAME + '_Model.txt', sep=\"\\t\")\n",
    "    print(\"Output Results can be found at the current directory!\")\n",
    "    print(\"Output Name is: \"+ 'Results_'+ NAME + '_Model.txt')\n",
    "    return data_prob_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file1 = open(\"myfile.txt\",\"w\") \n",
    "L = [\"ID\\t\",\"LANG\\n\"]  \n",
    "  \n",
    "# \\n is placed to indicate EOL (End of Line) \n",
    "\n",
    "file1.writelines([\"a\\t\",'b']) \n",
    "file1.close() #to change file access modes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words: 30142\n",
      "Number of Words: 34062\n",
      "Number of Words: 26913\n",
      "Output Results can be found at the current directory!\n",
      "Output Name is: Results_Word_Trigram_Model.txt\n"
     ]
    }
   ],
   "source": [
    "Path_Data='Data/'\n",
    "Train_Data_Set=['EN.txt', 'FR.txt', 'GR.txt']\n",
    "Path_test_data = Path_Data+'LangID.test.txt'\n",
    "All_prob=list()\n",
    "for name_train in Train_Data_Set:\n",
    "    Path_train_data = Path_Data+name_train\n",
    "    All_prob.append(test_with_word_model(Path_train_data, Path_test_data))\n",
    "\n",
    "\n",
    "NAME='Word_Trigram'\n",
    "output = Save_Results(All_prob[0],All_prob[1],All_prob[2], NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Path_Data='Data/'\n",
    "Train_Data_Set=['EN.txt', 'FR.txt', 'GR.txt']\n",
    "Path_Train_Data=Path_Data+Train_Data_Set[0]\n",
    "Path_test_data = Path_Data+'LangID.test.txt'\n",
    "\n",
    "sentence_list=return_sentence_list_from_file(Path_Train_Data)\n",
    "vocabulary = return_vocabulary_from_sentence_list(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words: 30142\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (4402, 4402, 4402) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1ab585497c85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mraw_unigram_word_counts\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mreturn_unigram_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mraw_bigram_word_counts\u001b[0m  \u001b[0;34m=\u001b[0m\u001b[0mreturn_bigram_word_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mraw_trigram_word_counts\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mreturn_Trigram_word_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#norm_bigram_word_counts = Norm_Bigram(raw_unigram_word_counts, raw_bigram_word_counts, vocabulary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-828dec662344>\u001b[0m in \u001b[0;36mreturn_Trigram_word_counts\u001b[0;34m(sentence_list, vocabulary)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreturn_Trigram_word_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mcount_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;31m#count_matrix+=1  ######## Add-one smoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mnum_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (4402, 4402, 4402) and data type float64"
     ]
    }
   ],
   "source": [
    "Path_Data='Data/'\n",
    "Train_Data_Set=['EN.txt', 'FR.txt', 'GR.txt']\n",
    "Path_Train_Data=Path_Data+Train_Data_Set[0]\n",
    "Path_test_data = Path_Data+'LangID.test.txt'\n",
    "\n",
    "sentence_list=return_sentence_list_from_file(Path_Train_Data)\n",
    "vocabulary = return_vocabulary_from_sentence_list(sentence_list)\n",
    "\n",
    "raw_unigram_word_counts =return_unigram_counts(sentence_list,vocabulary)\n",
    "raw_bigram_word_counts  =return_bigram_word_counts(sentence_list,vocabulary)\n",
    "raw_trigram_word_counts =return_Trigram_word_counts(sentence_list,vocabulary)\n",
    "#norm_bigram_word_counts = Norm_Bigram(raw_unigram_word_counts, raw_bigram_word_counts, vocabulary)\n",
    "\n",
    "norm_trigram_word_counts = Norm_Trigram(raw_unigram_word_counts, raw_bigram_word_counts, raw_trigram_word_counts, vocabulary)\n",
    "\n",
    "#Return_c_from_Good_Turing_Smoothing(raw_unigram_word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EN.txt', 'FR.txt', 'GR.txt', 'LangID.gold.txt', 'LangID.test.txt', 'Letter_Bigram_Model.py', 'Results_Letter_Bigram_Model.txt', 'Results_Word_Bigram_Good_Turing_Smoothing_Model.txt', 'Results_Word_Bigram_Model.txt', 'Results_Word_Trigram_Model.txt', 'run_all_python_scripts_for_HW1.sh', 'Word_Bigram_Model.py', 'Word_Bigram_Model_bp.py', 'Word_Bigram_Model_Good_Turing_Smoothing.py', 'Word_Trigram_Model.py']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LANG_ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID LANG_ref\n",
       "0  1.0       EN\n",
       "1  2.0       EN\n",
       "2  3.0       EN\n",
       "3  4.0       EN\n",
       "4  5.0       EN"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path='Data/'\n",
    "xx = os.listdir(Path)\n",
    "print (xx)\n",
    "df_letter = pd.read_csv(Path+'Results_Letter_Bigram_Model.txt', sep=\"\\t\").rename(columns={'LANG':'LANG_letter'})\n",
    "df_Word = pd.read_csv(Path+'Results_Word_Bigram_Model.txt', sep=\"\\t\").rename(columns={'LANG':'LANG_Word'})\n",
    "df_Word_Good = pd.read_csv(Path+'Results_Word_Bigram_Good_Turing_Smoothing_Model.txt', sep=\"\\t\").rename(columns={'LANG':'LANG_Word_Good_Turing'})\n",
    "df_reference = pd.read_csv(Path+'LangID.gold.txt', sep=\" \").rename(columns={'LANG':'LANG_ref'})\n",
    "df_trigram = pd.read_csv(Path+'Results_Word_Trigram_Model.txt', sep=\"\\t\").rename(columns={'LANG':'LANG_Trigram'})\n",
    "df_reference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LANG_ref</th>\n",
       "      <th>LANG_Word</th>\n",
       "      <th>LANG_Word_Good_Turing</th>\n",
       "      <th>LANG_letter</th>\n",
       "      <th>LANG_Trigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>GR</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>GR</td>\n",
       "      <td>GR</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID LANG_ref LANG_Word LANG_Word_Good_Turing LANG_letter LANG_Trigram\n",
       "0  1.0       EN        EN                    EN          EN           EN\n",
       "1  2.0       EN        EN                    GR          EN           EN\n",
       "2  3.0       EN        EN                    GR          GR           EN\n",
       "3  4.0       EN        EN                    EN          EN           EN\n",
       "4  5.0       EN        EN                    EN          EN           EN"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum = df_reference.merge(df_Word.merge(df_Word_Good.merge(df_letter.merge(df_trigram, on='ID'), on='ID'), on='ID'), on='ID')\n",
    "df_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8666666666666667\n",
      "0.9733333333333334\n",
      "0.92\n",
      "0.7733333333333333\n"
     ]
    }
   ],
   "source": [
    "print (len(df_sum[df_sum['LANG_ref']==df_sum['LANG_letter']])/150.0)\n",
    "\n",
    "print (len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Word']])/150.0)\n",
    "\n",
    "print (len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Word_Good_Turing']])/150.0)\n",
    "\n",
    "print (len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Trigram']])/150.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (4402, 4402, 4402) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-49f10290b624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#4402**3/1024**3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4402\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4402\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4402\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (4402, 4402, 4402) and data type float64"
     ]
    }
   ],
   "source": [
    "#4402**3/1024**3\n",
    "np.zeros((4402,4402,4402))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accuracy=[1.0, len(df_sum[df_sum['LANG_ref']==df_sum['LANG_letter']])/150.0, len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Word']])/150.0,\n",
    "        len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Word_Good_Turing']])/150.0, len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Trigram']])/150.0]\n",
    "Accuracy\n",
    "#df.append({'foo':1, 'bar':2}, ignore_index=True)\n",
    "df_sum.set_index('ID').append({'LANG_ref':1.0, 'LANG_Word':len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Word']])/150.0 , \n",
    "                               'LANG_Word_Good_Turing':len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Word_Good_Turing']])/150.0 ,\n",
    "                               'LANG_Trigram':len(df_sum[df_sum['LANG_ref']==df_sum['LANG_Trigram']])/150.0,\n",
    "                               'LANG_letter':len(df_sum[df_sum['LANG_ref']==df_sum['LANG_letter']])/150.0 \n",
    "                              },ignore_index=True).to_csv('All_Models_Summary.txt', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_lx",
   "language": "python",
   "name": "py3_lx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
