{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_file(file_path, num_rows):\n",
    "    ## num_rows has to be a 2 elements list\n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        row_range=range(num_rows[0],num_rows[1]+1)\n",
    "        i=0\n",
    "        for line in file:\n",
    "            if (i in row_range):               \n",
    "                   print (line)\n",
    "            i+=1\n",
    "    return None\n",
    "\n",
    "def filter_alphanumeric(word):\n",
    "#\\w matches any alphanumeric character\n",
    "    merge_words_no_digit=''\n",
    "    if (word!=''):\n",
    "        all_match = re.findall('\\w+', word)\n",
    "        all_match = list(filter(None, all_match))\n",
    "        merge_words=''\n",
    "        for item in all_match:\n",
    "            merge_words+=item\n",
    "        ## Find All unicode, then all non digits\n",
    "        merge_words=re.findall('[^(\\_|\\d)]', merge_words)\n",
    "        for item in merge_words:\n",
    "            merge_words_no_digit+=item        \n",
    "    else:\n",
    "        #print (\"Warning, One word is empty.\")\n",
    "        return '' \n",
    "    return merge_words_no_digit.lower()\n",
    "\n",
    "def filter_word(word):\n",
    "    return word.lower()\n",
    "\n",
    "def return_word_list_from_file(Path_File):\n",
    "    list_words=list()\n",
    "    with open(Path_File,  mode='r', newline='') as file:\n",
    "        for line in file:\n",
    "            for word in (re.split(\"\\s+\", line.rstrip('\\n'))):\n",
    "                if (word !=''):\n",
    "                    list_words.append(filter_alphanumeric(word))\n",
    "## if different languague, above line has to be changed\n",
    "    list_words = list(filter(None, list_words))\n",
    "    return list_words\n",
    "\n",
    "\n",
    "def return_sentence_list_from_file(Path_File):\n",
    "    list_sentences=list()\n",
    "    with open(Path_File,  mode='r', newline='') as file:\n",
    "        for line in file:\n",
    "            if (len(line)>1):\n",
    "                sentence = line.rstrip('\\n').rstrip('\\r')\n",
    "                #marked_sentence = #'<^> '+line.rstrip('\\n').rstrip('\\r')+' </s>'\n",
    "                list_sentences.append(sentence)\n",
    "    list_sentences = list(filter(None, list_sentences))\n",
    "    return list_sentences\n",
    "\n",
    "\n",
    "## not needed in HMM\n",
    "def return_vocabulary_from_sentence_list(sentence_list):\n",
    "    total_vocabulary=set({'^','$'})\n",
    "    for sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "        for word in word_list:\n",
    "            filtered_word = filter_alphanumeric(word)\n",
    "            if(filtered_word!=''):\n",
    "                total_vocabulary.add(filtered_word)\n",
    "    return sorted(list(total_vocabulary))\n",
    "\n",
    "def return_unigram_counts(sentence_list, vocabulary):\n",
    "    count_matrix =np.zeros((len(vocabulary)))\n",
    "    #count_matrix += len(vocabulary) ######## Add-one smoothing\n",
    "    ## Set value for sentence start <s>\n",
    "    count_matrix[vocabulary.index('^')] += len(sentence_list)\n",
    "    count_matrix[vocabulary.index('$')] += len(sentence_list)\n",
    "    for tem_sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", tem_sentence.rstrip('\\n'))\n",
    "        for word in word_list:\n",
    "            filtered_word = filter_word(word)\n",
    "            if(filtered_word in vocabulary):\n",
    "                word_index = vocabulary.index(filtered_word)\n",
    "                count_matrix[word_index]+=1\n",
    "    return count_matrix\n",
    "\n",
    "def return_word_from_sentence(sentence):\n",
    "    list_words=list()\n",
    "    for word in (re.split(\"\\s+\", sentence.rstrip('\\n'))):\n",
    "                    if (word !=''):\n",
    "                        list_words.append(filter_alphanumeric(word))\n",
    "    ## if different languague, above line has to be changed\n",
    "    list_words = list(filter(None, list_words))\n",
    "    return list_words\n",
    "\n",
    "def return_prob_of_test_sentence(test_words, P_likelihood_class):\n",
    "    bag_words = return_word_from_sentence(test_words)\n",
    "    Log_P_of_sentence=0\n",
    "    P_likelihood=P_likelihood_class\n",
    "        \n",
    "    for i in range(len(bag_words)):\n",
    "        if bag_words[i] in P_likelihood.keys():\n",
    "            Log_P_of_sentence+=np.log(P_likelihood[bag_words[i]])\n",
    "\n",
    "    return Log_P_of_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stop_Words=['the','a', 'and', 'i', 'it', 'is', 'to', 'a', 'of', 'this', 'with', 'for',\n",
    "                'you', 'that', 'in', 'have', 'my', 'on', 'as', 'but', 'use', 'are', 'phone',\n",
    "                'has', 'all', 'was', 'so', 'one', 'be', 'at', 'than', 'an']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_Folder='/mnt/e/Dropbox/GWU_Experience/Physics_CS_Study/CS/CS_NLP/HW/HW3/CR/txt/'\n",
    "\n",
    "Set_FileName=['neg.tok', 'pos.tok']\n",
    "\n",
    "sentence_list_neg   = return_sentence_list_from_file(PATH_Folder+Set_FileName[0])\n",
    "vocabulary_list_neg  = return_vocabulary_from_sentence_list(sentence_list_neg[0:])\n",
    "vocabulary_list_neg = list(set(vocabulary_list_neg) - set(Stop_Words))\n",
    "uni_counts_neg = return_unigram_counts(sentence_list_neg[0:], vocabulary_list_neg)\n",
    "\n",
    "sentence_list_pos  = return_sentence_list_from_file(PATH_Folder+Set_FileName[1])\n",
    "vocabulary_list_pos  = return_vocabulary_from_sentence_list(sentence_list_pos[0:])\n",
    "vocabulary_list_pos = list(set(vocabulary_list_pos) - set(Stop_Words))\n",
    "uni_counts_pos = return_unigram_counts(sentence_list_pos[0:], vocabulary_list_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_doc = uni_counts_neg.sum()+uni_counts_pos.sum()\n",
    "P_prior_neg = uni_counts_neg.sum()/N_doc\n",
    "P_prior_pos = uni_counts_pos.sum()/N_doc\n",
    "\n",
    "size_vocabulary = len(set(vocabulary_list_neg + vocabulary_list_pos)) -2\n",
    "P_likelihood_pos = (uni_counts_pos+1)/(N_doc+size_vocabulary)\n",
    "P_likelihood_neg = (uni_counts_neg+1)/(N_doc+size_vocabulary)\n",
    "\n",
    "P_likelihood_pos = dict(zip(vocabulary_list_pos, P_likelihood_pos))\n",
    "P_likelihood_neg = dict(zip(vocabulary_list_neg, P_likelihood_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003892640962260846\n",
      "0.0005838961443391269\n"
     ]
    }
   ],
   "source": [
    "print (P_likelihood_pos['good'])\n",
    "print (P_likelihood_neg['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_from_naive_bayes_classifier(test_sentence, P_prior_pos, P_likelihood_pos, P_prior_neg, P_likelihood_neg):\n",
    "    p_pos = return_prob_of_test_sentence(test_sentence, P_likelihood_pos) + np.log(P_prior_pos) +np.log(P_prior_pos)\n",
    "    #print (p_pos)\n",
    "    p_neg = return_prob_of_test_sentence(test_sentence, P_likelihood_neg) + np.log(P_prior_neg) +np.log(P_prior_neg)\n",
    "    #print (p_neg)\n",
    "    \n",
    "    if (p_pos<p_neg):\n",
    "        classification_result='+'\n",
    "    else:\n",
    "        classification_result='-'\n",
    "    return classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n",
      "0.7186422699549191\n",
      "F1: \n",
      "0.7268830075809879\n"
     ]
    }
   ],
   "source": [
    "test  ='but, if you are looking for my opinion of the apex dvd player , i love it !'\n",
    "test_1='bad'\n",
    "\n",
    "pos_count=0\n",
    "for test in sentence_list_pos[:]:\n",
    "    results = test_from_naive_bayes_classifier(test, P_prior_pos, P_likelihood_pos, P_prior_neg, P_likelihood_neg)\n",
    "    if (results=='+'):\n",
    "        pos_count+=1\n",
    "\n",
    "neg_count=0\n",
    "for test in sentence_list_neg[:]:\n",
    "    results = test_from_naive_bayes_classifier(test, P_prior_pos, P_likelihood_pos, P_prior_neg, P_likelihood_neg)\n",
    "    if (results=='-'):\n",
    "        neg_count+=1\n",
    "\n",
    "print (\"Accuracy: \")\n",
    "print ((pos_count + neg_count)/( len(sentence_list_neg) + len(sentence_list_pos) ))\n",
    "\n",
    "precision = (pos_count)/(len(sentence_list_pos))\n",
    "recall = (neg_count)/(len(sentence_list_neg))\n",
    "print (\"F1: \")\n",
    "print (2*precision*recall/(recall+precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.855724442539113\n",
      "-8.745088482612585\n"
     ]
    }
   ],
   "source": [
    "test  ='but , if you are looking for my opinion of the apex dvd player , i love it ! .'\n",
    "test_1='bad'\n",
    "test=test_1\n",
    "\n",
    "p_pos = np.log(P_prior_pos)+return_prob_of_test_sentence(test, P_likelihood_pos)\n",
    "print (p_pos)\n",
    "\n",
    "p_neg = np.log(P_prior_neg)+return_prob_of_test_sentence(test, P_likelihood_neg)\n",
    "print (p_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "#iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but the major problem i had was with the software .'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(sentence_list_neg, test_size=0.1, random_state=None)\n",
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)\n",
    "len(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
