{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_file(file_path, num_rows):\n",
    "    ## num_rows has to be a 2 elements list\n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        row_range=range(num_rows[0],num_rows[1]+1)\n",
    "        i=0\n",
    "        for line in file:\n",
    "            if (i in row_range):               \n",
    "                   print (line)\n",
    "            i+=1\n",
    "    return None\n",
    "\n",
    "def filter_alphanumeric(word):\n",
    "#\\w matches any alphanumeric character\n",
    "    merge_words_no_digit=''\n",
    "    if (word!=''):\n",
    "        all_match = re.findall('\\w+', word)\n",
    "        all_match = list(filter(None, all_match))\n",
    "        merge_words=''\n",
    "        for item in all_match:\n",
    "            merge_words+=item\n",
    "        ## Find All unicode, then all non digits\n",
    "        merge_words=re.findall('[^(\\_|\\d)]', merge_words)\n",
    "        for item in merge_words:\n",
    "            merge_words_no_digit+=item        \n",
    "    else:\n",
    "        #print (\"Warning, One word is empty.\")\n",
    "        return '' \n",
    "    return merge_words_no_digit.lower()\n",
    "\n",
    "def filter_word(word):\n",
    "    return word.lower()\n",
    "\n",
    "def return_word_list_from_file(Path_File):\n",
    "    list_words=list()\n",
    "    with open(Path_File,  mode='r', newline='') as file:\n",
    "        for line in file:\n",
    "            for word in (re.split(\"\\s+\", line.rstrip('\\n'))):\n",
    "                if (word !=''):\n",
    "                    list_words.append(filter_alphanumeric(word))\n",
    "## if different languague, above line has to be changed\n",
    "    list_words = list(filter(None, list_words))\n",
    "    return list_words\n",
    "\n",
    "\n",
    "def return_sentence_list_from_file(Path_File):\n",
    "    list_sentences=list()\n",
    "    with open(Path_File,  mode='r', newline='') as file:\n",
    "        for line in file:\n",
    "            if (len(line)>1):\n",
    "                sentence = line.rstrip('\\n').rstrip('\\r')\n",
    "                #marked_sentence = #'<^> '+line.rstrip('\\n').rstrip('\\r')+' </s>'\n",
    "                list_sentences.append(sentence)\n",
    "    list_sentences = list(filter(None, list_sentences))\n",
    "    return list_sentences\n",
    "\n",
    "\n",
    "## not needed in HMM\n",
    "def return_vocabulary_from_sentence_list(sentence_list):\n",
    "    total_vocabulary=set({'^','$'})\n",
    "    for sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", sentence.rstrip('\\n'))\n",
    "        for word in word_list:\n",
    "            filtered_word = filter_alphanumeric(word)\n",
    "            if(filtered_word!=''):\n",
    "                total_vocabulary.add(filtered_word)\n",
    "    return sorted(list(total_vocabulary))\n",
    "\n",
    "def return_unigram_counts(sentence_list, vocabulary):\n",
    "    count_matrix =np.zeros((len(vocabulary)))\n",
    "    #count_matrix += len(vocabulary) ######## Add-one smoothing\n",
    "    ## Set value for sentence start <s>\n",
    "    count_matrix[vocabulary.index('^')] += len(sentence_list)\n",
    "    count_matrix[vocabulary.index('$')] += len(sentence_list)\n",
    "    for tem_sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", tem_sentence.rstrip('\\n'))\n",
    "        for word in word_list:\n",
    "            filtered_word = filter_word(word)\n",
    "            if(filtered_word in vocabulary):\n",
    "                word_index = vocabulary.index(filtered_word)\n",
    "                count_matrix[word_index]+=1\n",
    "    return count_matrix\n",
    "\n",
    "def return_bigram_word_counts(sentence_list, vocabulary):\n",
    "    count_matrix=np.zeros((len(vocabulary),len(vocabulary)))\n",
    "    #count_matrix+=1  ######## Add-one smoothing\n",
    "    num_word=0\n",
    "    for tem_sentence in sentence_list:\n",
    "        word_list = re.split(\"\\s+\", tem_sentence.rstrip('\\n'))\n",
    "        word_list = list(filter(None, word_list))\n",
    "        \n",
    "        for i in range(0,len(word_list)-1):\n",
    "            num_word+=1\n",
    "            if (i==0):\n",
    "                first_word = filter_word(word_list[0])\n",
    "                if(first_word in vocabulary):\n",
    "                    count_matrix[vocabulary.index('^'), vocabulary.index(first_word)] += 1\n",
    "            tem_bigram_count_pairs = word_list[i:i+2]\n",
    "            first_word=filter_word(tem_bigram_count_pairs[0])\n",
    "            second_word=filter_word(tem_bigram_count_pairs[1])\n",
    "            \n",
    "            if((first_word in vocabulary) & (second_word in vocabulary)):\n",
    "                first_digit=vocabulary.index(first_word)\n",
    "                second_digit=vocabulary.index(second_word)\n",
    "                count_matrix[first_digit,second_digit]+=1\n",
    "    print (\"Number of Words: \"+ str(num_word))\n",
    "    return count_matrix\n",
    "\n",
    "\n",
    "def return_word_from_sentence(sentence):\n",
    "    list_words=list()\n",
    "    for word in (re.split(\"\\s+\", sentence.rstrip('\\n'))):\n",
    "                    if (word !=''):\n",
    "                        list_words.append(filter_alphanumeric(word))\n",
    "    ## if different languague, above line has to be changed\n",
    "    list_words = list(filter(None, list_words))\n",
    "    return list_words\n",
    "\n",
    "def return_prob_of_test_sentence(test_words, P_likelihood_class):\n",
    "    bag_words = return_word_from_sentence(test_words)\n",
    "    Log_P_of_sentence=0\n",
    "    P_likelihood=P_likelihood_class\n",
    "        \n",
    "    for i in range(len(bag_words)):\n",
    "        if bag_words[i] in P_likelihood.keys():\n",
    "            Log_P_of_sentence+=np.log(P_likelihood[bag_words[i]])\n",
    "\n",
    "    return Log_P_of_sentence\n",
    "\n",
    "def test_from_naive_bayes_classifier(test_sentence, P_prior_pos, P_likelihood_pos, P_prior_neg, P_likelihood_neg):\n",
    "    p_pos = return_prob_of_test_sentence(test_sentence, P_likelihood_pos) + np.log(P_prior_pos) +np.log(P_prior_pos)\n",
    "    #print (p_pos)\n",
    "    p_neg = return_prob_of_test_sentence(test_sentence, P_likelihood_neg) + np.log(P_prior_neg) +np.log(P_prior_neg)\n",
    "    #print (p_neg)\n",
    "    \n",
    "    if (p_pos<p_neg):\n",
    "        classification_result='+'\n",
    "    else:\n",
    "        classification_result='-'\n",
    "    return classification_result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stop_Words=['the','a', 'and', 'i', 'it', 'is', 'to', 'a', 'of', 'this', 'with', 'for',\n",
    "                'you', 'that', 'in', 'have', 'my', 'on', 'as', 'but', 'use', 'are', 'phone',\n",
    "                'has', 'all', 'was', 'so', 'one', 'be', 'at', 'than', 'an']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words: 25099\n",
      "Number of Words: 39765\n",
      "Number of Words: 24992\n",
      "Number of Words: 40209\n",
      "Number of Words: 24766\n",
      "Number of Words: 40086\n",
      "Number of Words: 24971\n",
      "Number of Words: 39858\n",
      "Number of Words: 25283\n",
      "Number of Words: 39938\n",
      "Number of Words: 25032\n",
      "Number of Words: 39874\n",
      "Number of Words: 24876\n",
      "Number of Words: 40068\n",
      "Number of Words: 24932\n",
      "Number of Words: 40211\n",
      "Number of Words: 24601\n",
      "Number of Words: 39751\n",
      "Number of Words: 24906\n",
      "Number of Words: 40011\n",
      "Accuracy: \n",
      "0.7930257226199946\n",
      "F1: \n",
      "0.8028226282325559\n"
     ]
    }
   ],
   "source": [
    "PATH_Folder='/mnt/e/Dropbox/GWU_Experience/Physics_CS_Study/CS/CS_NLP/HW/HW3/CR/txt/'\n",
    "\n",
    "Set_FileName=['neg.tok', 'pos.tok']\n",
    "\n",
    "Total_accuracy=0\n",
    "Total_F1=0\n",
    "for i in range(10):\n",
    "    sentence_list_neg   = return_sentence_list_from_file(PATH_Folder+Set_FileName[0])\n",
    "    X_train, X_test = train_test_split(sentence_list_neg, test_size=0.1, random_state=None)\n",
    "    vocabulary_list_neg  = return_vocabulary_from_sentence_list(X_train)\n",
    "    vocabulary_list_neg = list(set(vocabulary_list_neg) - set(Stop_Words))\n",
    "    bi_counts_neg = return_bigram_word_counts(X_train, vocabulary_list_neg)\n",
    "\n",
    "    sentence_list_pos  = return_sentence_list_from_file(PATH_Folder+Set_FileName[1])\n",
    "    X_train, X_test = train_test_split(sentence_list_pos, test_size=0.1, random_state=None)\n",
    "    vocabulary_list_pos  = return_vocabulary_from_sentence_list(X_train)\n",
    "    vocabulary_list_pos = list(set(vocabulary_list_pos) - set(Stop_Words))\n",
    "    bi_counts_pos = return_bigram_word_counts(X_train, vocabulary_list_pos)\n",
    "\n",
    "\n",
    "    N_doc = uni_counts_neg.sum()+uni_counts_pos.sum()\n",
    "    P_prior_neg = bi_counts_neg.sum()/N_doc\n",
    "    P_prior_pos = bi_counts_pos.sum()/N_doc\n",
    "\n",
    "    size_vocabulary  = len(set(vocabulary_list_neg + vocabulary_list_pos)) -2\n",
    "    P_likelihood_pos = (uni_counts_pos+1)/(N_doc+size_vocabulary)\n",
    "    P_likelihood_neg = (uni_counts_neg+1)/(N_doc+size_vocabulary)\n",
    "\n",
    "    P_likelihood_pos = dict(zip(vocabulary_list_pos, P_likelihood_pos))\n",
    "    P_likelihood_neg = dict(zip(vocabulary_list_neg, P_likelihood_neg))\n",
    "\n",
    "\n",
    "    pos_count=0\n",
    "    for test in sentence_list_pos[:]:\n",
    "        results = test_from_naive_bayes_classifier(test, P_prior_pos, P_likelihood_pos, P_prior_neg, P_likelihood_neg)\n",
    "        if (results=='+'):\n",
    "            pos_count+=1\n",
    "\n",
    "    neg_count=0\n",
    "    for test in sentence_list_neg[:]:\n",
    "        results = test_from_naive_bayes_classifier(test, P_prior_pos, P_likelihood_pos, P_prior_neg, P_likelihood_neg)\n",
    "        if (results=='-'):\n",
    "            neg_count+=1\n",
    "\n",
    "    Total_accuracy+= ((pos_count + neg_count)/( len(sentence_list_neg) + len(sentence_list_pos) ))\n",
    "\n",
    "    precision = (pos_count)/(len(sentence_list_pos))\n",
    "    recall = (neg_count)/(len(sentence_list_neg))\n",
    "\n",
    "    Total_F1+=(2*precision*recall/(recall+precision))\n",
    "    \n",
    "print (\"Accuracy: \")\n",
    "print (Total_accuracy/10.0)\n",
    "print (\"F1: \")\n",
    "print (Total_F1/10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words: 40011\n"
     ]
    }
   ],
   "source": [
    "bi_counts_neg = return_bigram_word_counts(X_train, vocabulary_list_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3136"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_counts_neg.shape\n",
    "len(bi_counts_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6882"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pos=list(np.zeros(len(bi_counts_pos)))\n",
    "y_neg=list(np.zeros(len(bi_counts_neg)))\n",
    "len(y_neg+y_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.855724442539113\n",
      "-8.745088482612585\n"
     ]
    }
   ],
   "source": [
    "test  ='but , if you are looking for my opinion of the apex dvd player , i love it ! .'\n",
    "test_1='bad'\n",
    "test=test_1\n",
    "\n",
    "p_pos = np.log(P_prior_pos)+return_prob_of_test_sentence(test, P_likelihood_pos)\n",
    "print (p_pos)\n",
    "\n",
    "p_neg = np.log(P_prior_neg)+return_prob_of_test_sentence(test, P_likelihood_neg)\n",
    "print (p_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "#iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but the major problem i had was with the software .'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(sentence_list_neg, test_size=0.1, random_state=None)\n",
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)\n",
    "len(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
